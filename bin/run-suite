#!/usr/bin/env python3

import os
import os.path
import subprocess
import sys
import copy
import timeit
import io
import argparse

app_list = [
      "2DConvolution",
      "2mm",
      "3DConvolution",
      "3mm",
      "arith",
      "atax",
      "bicg",
      "blocked_transform",
      "correlation",
      "covariance",
      "dag_task_throughput_independent",
      "dag_task_throughput_sequential",
      "DRAM",
      "fdtd2d",
      "gemm",
      "gesummv",
      "gramschmidt",
      "host_device_bandwidth",
      "kmeans",
      "lin_reg_coeff",
      "lin_reg_error",
      "local_mem",
      "matmulchain",
      "median",
      "mol_dyn",
      "mvt",
      "nbody",
      "pattern_L2",
      "reduction",
      "scalar_prod",
      "segmentedreduction",
      "sf",
      "sobel",
      "sobel5",
      "sobel7",
      "syr2k",
      "syrk",
      "vec_add"
    ]
app_list.sort()

known_fail = {"3DConvolution",
      "DRAM",
      "blocked_transform"}

def create_linear_range(begin, end, numsteps):
  stepsize = (end-begin) // numsteps
  if stepsize == 0:
    stepsize = 1
    
  return [begin+i*stepsize for i in range(0,numsteps)]

def create_log_range(begin, end):
  result = [begin]
  current = begin
  while current < end:
    current *= 2
    result += [current]
    
  return result


''' supported options of benchmarks:
    --size=<problem-size> - total problem size. For most benchmarks, global range of work items. Default: 3072
    --local=<local-size> - local size/work group size, if applicable. Not all benchmarks use this. Default: 256
    --num-runs=<N> - the number of times that the problem should be run, e.g. for averaging runtimes. Default: 5
    --device=<d> - changes the SYCL device selector that is used. Supported values: cpu, gpu, default. Default: default
    --output=<output> - Specify where to store the output and how to format. If <output>=stdio, results are printed to standard output. For any other value, <output> is interpreted as a file where the output will be saved in csv format.
    --verification-begin=<x,y,z> - Specify the start of the 3D range that should be used for verifying results. Note: Most benchmarks do not implement this feature. Default: 0,0,0
    --verification-range=<x,y,z> - Specify the size of the 3D range that should be used for verifying results. Note: Most benchmarks do not implement this feature. Default: 1,1,1
    --no-verification - disable verification entirely
    --no-ndrange-kernels - do not run kernels based on ndrange parallel for
    --hierarchical-kernels - run kernels using hierarchical parallelism
'''
output_file = "./sycl-bench.csv"
discard = io.StringIO()

parser = argparse.ArgumentParser(description="Run benchmarks",
                                 formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument("profile", metavar="profile", choices={'default', 'quicktest', 'cpu', 'gpu', 'cpu-noverify', 'cpu-nondrange', 'cpu-noverify-nondrange', 'gpu-noverify'})
parser.add_argument("-log", metavar="outfile",
                    default=sys.stderr,
                    type=argparse.FileType('w', encoding='UTF-8'),
                    help="Log file")
parser.add_argument("--try-fail", action='store_true', help="Try known failures")
parser.add_argument("-name", metavar="NAME", default=None,
                    help="Run name")
parser.add_argument("-r", "--runs", metavar="NB_RUNS",
                    default=10, type=int,
                    help="Number of runs")
parser.add_argument("-o",
                    metavar="OUT_FILE",
                    default=output_file,
                    help="Out file")
parser.add_argument("-t", "--timeout", metavar="SEC",
                    default=-1,
                    help="Time out (second)", type=int)

parse_args = parser.parse_args()
if parse_args.timeout <= 0:
  parse_args.timeout = None

output_file = parse_args.o

default_profile = {
  # Problem size will not be increased if this runtime is exceeded
  'max-allowed-runtime' : 10800.0, #seconds.
  'default-options' : {
    '--size' : create_log_range(2**10, 2**10),
    '--local' : create_log_range(256,256),
    '--num-runs' : parse_args.runs,
    '--output' : output_file
  },
  'default-flags' : set([]),
  'individual-benchmark-options' : {
    'pattern_L2' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'pattern_arith' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'pattern_sf' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'pattern_shared' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'kmeans' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'lin_reg_coeff' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'lin_reg_error' : {
      '--size' : create_log_range(2**16, 2**16)
    },
    'mol_dyn' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'scalar_prod' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'vec_add' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'blocked_transform' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'dag_task_throughput_independent' : {
      '--size' : create_log_range(2**10, 2**16)
    },
    'dag_task_throughput_sequential' : {
      '--size' : create_log_range(2**10, 2**16)
    },
    'reduction' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    'segmentatedreduction' : {
      '--size' : create_log_range(2**20, 2**20)
    },
    '2DConvolution' : {
      '--size' : create_log_range(2**12, 2**12)
    },
    'atax' : {
      '--size' : create_log_range(2**12, 2**12)
    },
    'bicg' : {
      '--size' : create_log_range(2**14, 2**14)
    },
    'gesummv' : {
      '--size' : create_log_range(2**14, 2**14)
    },
    'mvt' : {
      '--size' : create_log_range(2**14, 2**14)
    },                                                                             
  },
  'individual-benchmark-flags' : set([])
}

def construct_profile(overridden_options_dict,
                      additional_flags=[],
                      max_allowed_runtime=default_profile['max-allowed-runtime']):

  new_profile = copy.deepcopy(default_profile)
  new_profile['max-allowed-runtime'] = max_allowed_runtime
  for opt in overridden_options_dict:
    new_profile['default-options'][opt] = overridden_options_dict[opt]
  
  for f in additional_flags:
    new_profile['default-flags'].add(f)
  
  return new_profile

profiles = {
  'default': default_profile,
  'quicktest' : construct_profile({}, max_allowed_runtime=1.0),
  'cpu' : construct_profile({'--device':'cpu'}),
  'gpu' : construct_profile({'--device':'gpu'}),
  'cpu-noverify' : construct_profile({'--device':'cpu'},['--no-verification']),
  'cpu-nondrange' : construct_profile({'--device':'cpu'},['--no-ndrange-kernels']),
  'cpu-noverify-nondrange' : construct_profile({'--device':'cpu'},['--no-verification','--no-ndrange-kernels']),
  'gpu-noverify' : construct_profile({'--device':'gpu'},['--no-verification'])
}


def invoke_benchmark(benchmark_executable, args):
  parse_args.log.write("__________________________________________________\n")
  parse_args.log.write("{} {}\n".format(os.path.basename(benchmark_executable), " ".join(args)))

  try:
    start = timeit.default_timer()
    retcode = subprocess.call([benchmark_executable]+args, timeout=parse_args.timeout)
    stop  = timeit.default_timer()

    elapsed_time = stop - start
  except subprocess.TimeoutExpired:
    retcode = 42
    parse_args.log.write("Time out\n")
    bench_run = list([get_basic_benchdata(True, False, "", "Time out")])
  if(retcode != 0):
    parse_args.log.write("==> Benchmark FAILED: {} with args {}\n".format(benchmark_executable,args))
  else:
    parse_args.log.write("==> Benchmark run finished in {} s\n".format(elapsed_time))

  return retcode, elapsed_time

def is_benchmark(filepath):

  filename, extension = os.path.splitext(filepath)

  # Yeah, so this is a bit of a hack.. the better solution would be
  # to have cmake generate a list of the actual benchmarks instead
  # of just trying to execute everything in this directory that looks
  # like a program
  if extension != '' and extension != '.exe' and extension != '.out':
    return False
  
  return True

if __name__ == '__main__':
  install_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)),"benchmarks")
  print(parse_args)
  profilename = parse_args.profile
  
  parse_args.log.write("Using test profile: " + profilename + "\n")
  profile = profiles[profilename]
  
  max_allowed_runtime = profile['max-allowed-runtime']
  default_options     = profile['default-options']
  default_flags       = profile['default-flags']
  # these are used to override arguments for invidual benchmarks
  individual_benchmark_options = profile['individual-benchmark-options']
  individual_benchmark_flags   = profile['individual-benchmark-flags']
  
  if os.path.exists(output_file):
    print("Error: output file {} already exists!".format(output_file))
    sys.exit(-1)

  with open(output_file, "w", encoding='UTF-8') as f:
      f.write(parse_args.name + "\n")
      f.write(parse_args.profile + "\n")

  failed_benchmarks = []

  for filename in app_list:
    if not parse_args.try_fail:
      if filename in known_fail:
        continue
    benchmark_name = filename
    benchmark_executable = os.path.realpath(os.path.join(install_dir,filename))
    if is_benchmark(benchmark_executable):

      parse_args.log.write("\n\n##################################################\n")
      parse_args.log.write("Processing: " + benchmark_name + "\n")
      parse_args.log.write("##################################################\n")

      flags = copy.deepcopy(default_flags)
      options = copy.deepcopy(default_options)

      # Overwrite default options with values that may be specified
      # for individual benchmarks
      if benchmark_name in individual_benchmark_options:
        for param in individual_benchmark_options[benchmark_name]:
          options[param] = individual_benchmark_options[benchmark_name][param]
      if benchmark_name in individual_benchmark_flags:
        for f in individual_benchmark_flags:
          flags.add(f)

      max_runtime = 0.0
      run_has_failed = False
      for size in options['--size']:
        parse_args.log.write("max_runtime " + str(max_runtime) + ", max_allowed_runtime " + str(max_allowed_runtime) + "\n")
        if max_runtime < max_allowed_runtime:
          for localsize in options['--local']:
            # some benchmarks may not work if problem size is not multiple of
            # local size.
            # Additionally, skip this benchmark if a run has failed - this may
            # indicate out of memory or some setup issue
            if size % localsize == 0 and not run_has_failed:

              args = []

              for f in flags:
                args.append(str(f))
              for arg in options:
                if not isinstance(options[arg], list):
                  args.append(str(arg)+'='+str(options[arg]))
              args.append('--size='+str(size))
              args.append('--local='+str(localsize))

              retcode, elapsed_time = invoke_benchmark(benchmark_executable, args)
              if retcode == 0:
                max_runtime = max(max_runtime, elapsed_time)
              else:
                run_has_failed = True
                failed_benchmarks.append(benchmark_name)
                parse_args.log.write("Benchmark failed, aborting run\n")

  if len(failed_benchmarks)==0:
    parse_args.log.write("All benchmarks were executed successfully\n")
    parse_args.log.close()
    sys.exit(0)
  else:
    parse_args.log.write("The following benchmarks were aborted because they "
          "returned a non-zero returncode:\n" + " ".join(failed_benchmarks) + "\n")
    parse_args.log.close()
    sys.exit(-1)
              
